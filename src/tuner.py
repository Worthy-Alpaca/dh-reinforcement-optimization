import torch
import optuna
import os
import pickle
import torch.nn as nn
from pathlib import Path
from typing import Literal, Union
from os.path import exists
from optuna.study import MaxTrialsCallback
from optuna.trial import TrialState
import random
import numpy as np

PATH = Path(os.getcwd() + os.path.normpath("/data/models"))

from main import RunModel


class Tuner:
    def __init__(
        self,
        direction: Literal["minimize", "maximize"] = "maximize",
        sampler: optuna.samplers = optuna.samplers.TPESampler,
        pruner: optuna.pruners = optuna.pruners.PercentilePruner,
    ) -> optuna.study:
        """Class to initiate a Model tuning session.
        Args:
            dataPath (Path): Path to a data source. May change to DB connection
            epochs (int): Number of epochs in each Trial run
            direction (Literal[&quot;minimize&quot;, &quot;maximize&quot;], optional): Direction to optimize. &quot;minimize&quot; optimizes Loss,  &quot;maximize&quot; optimizes Accuracy. Defaults to "maximize".
            sampler (optuna.samplers, optional): Optuna Sampler Algorythm to use. Defaults to optuna.samplers.TPESampler.
            pruner (optuna.pruners, optional): Optuna Pruner Algorythm to use. Defaults to optuna.pruners.HyperbandPruner.
        Returns:
            optuna.study: Optuna Study like session.
        """
        self.study = optuna.create_study(
            load_if_exists=True,
            study_name="optimizer_100_final_2",
            direction=direction,
            sampler=sampler(),
            pruner=optuna.pruners.HyperbandPruner(),
            storage="sqlite:///50Studies_p4.db",
        )

    def optimize(
        self,
        n_trials: int,
    ) -> optuna.trial.FrozenTrial:
        """Starts the optimization process.
        Args:
            n_trials (int): Number of trials to run.
        Returns:
            optuna.trial.FrozenTrial: The best trial according to optimizer.
        """
        self.study.optimize(
            self.__objective,
            n_trials=n_trials,
            catch=(
                RuntimeError,
                RuntimeWarning,
                TypeError,
                ValueError,
                AssertionError,
                OSError,
            ),
            callbacks=[
                self.__logging_callback,
                MaxTrialsCallback(
                    n_trials, states=(TrialState.COMPLETE, TrialState.PRUNED)
                ),
            ],
        )

        return self.study.best_trial

    def __objective(self, trial: optuna.trial) -> float:
        """Generate the study objectives.
        Args:
            trial (optuna.trial): The current trial.
        Raises:
            TypeError: Unknown Direction. Shouldn't occur. Sanity check.
        Returns:
            float: The current objective. Either accuracy or loss
        """
        params = {
            "learning_rate": trial.suggest_float("learning_rate", 0.003, 0.005),
            "optimizer": trial.suggest_categorical("optimizer", ["Adam"]),
            "loss_function": trial.suggest_categorical(
                "loss_function",
                [
                    # "FocalTverskyLoss",
                    # "TverskyLoss",
                    "L1Loss",
                    "HuberLoss",
                ],
            ),
            "batch_size": trial.suggest_int("batch_size", 22, 24),
            "embed_dimensions": trial.suggest_int("embed_dimensions", 15, 30),
            "embed_iterations": trial.suggest_int("embed_iterations", 2, 3),
            "weight_decay": trial.suggest_float("weight_decay", 0.04, 0.05),
            "dampening": trial.suggest_float("dampening", 0.48, 0.52),
            "momentum": trial.suggest_float("momentum", 0.28, 0.32),
            "gamma": trial.suggest_float("gamma", 0.64, 0.66),
            "min_epsilon": trial.suggest_float("min_epsilon", 0.2, 0.8),
            "epsilon_decay": trial.suggest_float("epsilon_decay", 0.4, 0.6),
        }

        loss = self.tuneModel(params, trial)

        return loss

    def tuneModel(
        self, params: dict, trial: optuna.trial = None, saveState: bool = False
    ) -> tuple[Union[torch.tensor, float], Union[torch.Tensor, float]]:
        """Tunes the model accoring to objective parameters.
        Args:
            params (dict): Paramters generated by Objective.
            trial (optuna.trial): Current optuna trial. Defaults to None.
            saveState (bool): If the parameters should be saved to file. Defaults to False.
        Returns:
            tuple[torch.tensor  float, torch.Tensor  float]: Mean Loss and Accuracy for validation.
        """
        if trial is not None:
            print("Starting Trial: ", trial.number)
        params["loss_function"] = getattr(nn, params["loss_function"])

        # random.seed(1000)
        # np.random.seed(1000)
        # torch.manual_seed(1000)

        optim_args = {
            "weight_decay": params["weight_decay"],
            "eps": params["momentum"],
            "dampening": params["momentum"],
        }

        runmodel = RunModel(
            dbpath=r"V:\transfer\SAP\SMD\SMD_Material_Stueli.txt",
            numSamples=params["batch_size"],
            tuning=False,
            allowDuplicates=False,
        )
        try:
            Q_Function, QNet, Adam, ExponentialLR = runmodel.init_model(
                # fname=os.path.join(runmodel.folder_name, shortest_fname),
                EMBEDDING_DIMENSIONS=params["embed_dimensions"],
                EMBEDDING_ITERATIONS_T=params["embed_iterations"],
                OPTIMIZER=getattr(torch.optim, params["optimizer"]),
                optim_args=optim_args,
                loss_func=params["loss_function"],
            )
        except:
            Q_Function, QNet, Adam, ExponentialLR = runmodel.init_model(
                # fname=os.path.join(runmodel.folder_name, shortest_fname),
                EMBEDDING_DIMENSIONS=params["embed_dimensions"],
                EMBEDDING_ITERATIONS_T=params["embed_iterations"],
                OPTIMIZER=getattr(torch.optim, params["optimizer"]),
                loss_func=params["loss_function"],
            )

        median_loss = runmodel.fit(
            Q_func=Q_Function,
            Q_net=QNet,
            optimizer=Adam,
            lr_scheduler=ExponentialLR,
            NR_EPISODES=3000,
            MIN_EPSILON=params["min_epsilon"],
            EPSILON_DECAY_RATE=params["epsilon_decay"],
            N_STEP_QL=4,
            BATCH_SIZE=params["batch_size"],
            GAMMA=0.7,
            trial=trial,
            debug=True,
        )

        if saveState:
            runmodel.plotMetrics()

        return median_loss

    def saveBestTrial(self, params: dict, path: Path = PATH):
        """Save the state and Parameters of the best trial.
        Args:
            params (optuna.trial.FrozenTrial): The best trial as determined by optuna
            path (Path, optional): Path to saving location. Defaults to PATH.
        """

        if not exists(path / "updatedModelUnscaled"):
            os.makedirs(path / "updatedModelUnscaled")
        with open(path / "updatedModelUnscaled" / "modelParameters.p", "wb") as fp:
            pickle.dump(params, fp, protocol=pickle.HIGHEST_PROTOCOL)

    def saveStudy(self, path: Path):
        """Saves the study to the provided path.
        Args:
            path (Path): Path to saving location.
        """
        if not exists(path):
            os.mkdir(path)
        with open(path / f"{self.study.study_name}.p", "wb") as fp:
            print(f"Saving Study with Name: {self.study.study_name}")
            pickle.dump(self.study, fp, protocol=pickle.HIGHEST_PROTOCOL)

    def loadStudy(self, path: Path) -> optuna.study.Study:
        """Load a study from the provided location.
        Args:
            path (Path): The location of the study.
        Returns:
            optuna.study.Study: The loaded study.
        """
        with open(path, "rb") as file:
            data = pickle.load(file)
        return data

    def __logging_callback(
        self, study: optuna.study.Study, frozen_trial: optuna.trial.FrozenTrial
    ):
        """Internal method used to save the current best trial.
        Args:
            study (optuna.study.Study): The current study.
            frozen_trial (optuna.trial.FrozenTrial): The current best trial.
        """
        previous_best_value = study.user_attrs.get("previous_best_value", None)
        if previous_best_value == None:
            return
        if previous_best_value != study.best_value:
            study.set_user_attr("previous_best_value", study.best_value)
            print(
                "Trial {} finished with best value: {} and parameters: {}. ".format(
                    frozen_trial.number,
                    frozen_trial.value,
                    frozen_trial.params,
                )
            )
            self.saveBestTrial(frozen_trial.params)


if __name__ == "__main__":
    import os

    try:
        torch.multiprocessing.set_start_method("spawn")
    except RuntimeError:
        pass
    STUDY_PATH = Path(os.getcwd() + os.path.normpath("/data/model/studies/50TrialsP1"))

    tuner = Tuner(direction="minimize")
    # best_trial = tuner.optimize(n_trials=100)

    study = optuna.load_study(
        storage="sqlite:///50studies_p4 - Kopie.db", study_name="optimizer_100_final"
    )

    params = study.best_trial.params
    print(params)

    tuner.tuneModel(params, None, True)

    # tuner.saveStudy(STUDY_PATH)
    # study = tuner.loadStudy(STUDY_LOAD)
